{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습1] 토큰화와 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 토큰화(Tokenizing): 글, 문단, 문장 따위를 그 하위 요소로 분절하는 것\n",
    "* 형태소 분석(morpheme analysis): 토큰화 중 문장을 의미가 있는 요소 별로 분절하여 분석하는 것\n",
    "* Twitter 모듈: konlpy에 속한 여러 형태소 분석기 중 하나\n",
    "* from konlpy.tag import Twitter: 한국어 자연어 처리를 위한 konly 라이브러리에서 Twitter 모듈 불러오기 \n",
    "* Twitter.morphs(sentence): 한국어 문장인 sentence를 형태소에 따라 분할\n",
    "* Twitter.pos(sentence): 한국어 문장인 sentence를 형태소와 품사에 따라 분할(POS-tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Twitter \n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "'문서'를 '문단'의 리스트로 변환하는 함수 doc2para를 완성합니다.\n",
    "\n",
    "   Step01. 문장의 마침표(.) 뒤에 있는 개행 표시(\\n)를 기준으로 \n",
    "           문서 내 글들을 리스트 요소 즉, 문단으로 나눕니다.\n",
    "           \n",
    "   Step02. 나누어진 글들 중 마지막 글자가 \".\"인 경우만 \n",
    "           문단이 나누어진 것으로 보고 그 외의 문장들은 서로 \n",
    "           병합하여 줍니다.\n",
    "'''\n",
    "\n",
    "def doc2para(writing): # writing: 개행문자가 엉망인 문서 \n",
    "    \n",
    "    paragraphs = []\n",
    "    \n",
    "    splited = writing.split(\"\\n\") # 일단 개행문자 기준으로 문서 쪼개기(리스트 형태) \n",
    "    para = \"\"\n",
    "    \n",
    "    for i in splited:\n",
    "        if len(i) == 0: continue # 빈 문자열일 경우 continue\n",
    "        elif i[-1] == \".\": # 마침표를 기준으로 문장을 나누기\n",
    "            para += i\n",
    "            paragraphs.append(para) # 마침표로 끝날 경우 문단에 추가\n",
    "            para = \"\" # 다시 para 초기화  \n",
    "        else: # 그 외는 서로 병합\n",
    "            para += i\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "'문단'을 '문장'의 리스트로 변환하는 함수 para2sen을 완성합니다.\n",
    "\n",
    "   Step01. 문단을 \".\"으로 나누어 리스트로 만들고, \n",
    "           변수 sentences에 저장합니다.\n",
    "           \n",
    "   Step02. sentences 내 문장들에 대해서 \"?\"로 재분할 한 후, \n",
    "           ndarray.flatten()을 활용하여 재분할된 문장이 합쳐질 \n",
    "           수 있도록 리스트로 만들어 줍니다. \n",
    "           (\"!\"에 대해서도 마찬가지로 적용합니다.)\n",
    "'''\n",
    "\n",
    "def para2sen(paragraph):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # Step01.\n",
    "    sentences = paragraph.split('.') # 일단 마침표를 기준으로 문장 나누기\n",
    "    \n",
    "    # Step02. \n",
    "\n",
    "    # 나눠진 문장을 다시 물음표를 기준으로 문장 나누기\n",
    "    sentences = np.array([s.split('?') for s in sentences])\n",
    "    # 모두 일반 리스트 형태로(겹리스트 제거)\n",
    "    sentences = sentences.flatten() \n",
    "    # 나눠진 문장을 다시 느낌표를 기준으로 문장 나누기\n",
    "    sentences = np.array([s.split('!') for s in sentences])\n",
    "    # 모두 일반 리스트 형태로(겹리스트 제거)\n",
    "    sentences = sentences.flatten() \n",
    "    \n",
    "    sentences = [sentence.replace('\"','') for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# 띄어쓰기로 문장을 구분하는 함수\n",
    "\n",
    "def sen2words_byspace(sentence):\n",
    "    \n",
    "    words = []\n",
    "    words = sentence.strip().split(\" \")\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 3번\n",
    "    'Twitter()'로 선언된 Tokenizer인 'analyzer'를 이용해 형태소에 따라 \n",
    "    분할된 문장의 리스트를 변수 'morphs'에 저장하는 sen2morph\n",
    "    함수를 완성합니다. Twitter.morphs 메소드를 사용하세요.\n",
    "'''\n",
    "\n",
    "def sen2morph(sentence):\n",
    "    \n",
    "    morphs = []\n",
    "    \n",
    "    analyzer = Twitter()\n",
    "    morphs = analyzer.morphs(sentence)\n",
    "    \n",
    "    return morphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 4번\n",
    "   3번과 같이 'Twitter()'로 선언된 Tokenizer인 'analyzer'를 이용해\n",
    "   형태소와 그에 따른 품사를 분할하는 analyzing_morphs 함수를 완성합니다.\n",
    "   Twitter.pos 메소드를 사용하세요.\n",
    "'''\n",
    "\n",
    "def analyzing_morphs(sentence):\n",
    "    \n",
    "    analyzer = Twitter()\n",
    "    \n",
    "    return analyzer.pos(sentence)\n",
    "    \n",
    "# 위에서 정의한 함수들을 바탕으로 문서를 토큰화를 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    DATA_PATH = \"./data/blood_rain.txt\"\n",
    "    \n",
    "    blood_rain = load_data(DATA_PATH)\n",
    "    paragraphs = doc2para(blood_rain)\n",
    "    sentences = para2sen(paragraphs[4])\n",
    "    words_byspace = sen2words_byspace(sentences[3])\n",
    "    words_bymorphs = sen2morph(sentences[3])\n",
    "    morphs_analyzed = analyzing_morphs(sentences[3])\n",
    "    \n",
    "    # 출력을 통해 토큰화가 잘 되었는지 확인합니다.\n",
    "    \n",
    "    print(\"문장으로 구분된 5번째 문단: \", sentences)\n",
    "    print(\"\\n띄어쓰기로 구분된 문장 (5번째 문단의 4번째 문장): \", words_byspace)\n",
    "    print(\"\\n형태소 별로 구분된 문장 (5번째 문단의 4번째 문장): \", words_bymorphs)\n",
    "    print(\"\\n형태소와 그에 따른 품사로 분류된 문장 (5번째 문단의 4번째 문장): \", morphs_analyzed)\n",
    "    \n",
    "    return words_byspace, words_bymorphs, morphs_analyzed\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cf. 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([[1,2],[3,4]])\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([list([1, 2]), list([3, 4]), list([5])], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2],[3,4],[5]]) # 오류; 이미 flatten이 되었다고 봄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cf. list of lists를 flatten 하는 법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [[1,2],[3,4],[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방법1 \n",
    "\n",
    "[elem for sublist in lists for elem in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 방법2\n",
    "\n",
    "lists = [[1,2],[3,4]]\n",
    "sum(lists,[]) # []를 가지고 sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습2] BoW, TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BoW(Bag of Words): 토큰화된 단어들을 해당 글에서 등장 횟수와 대응시켜 놓은 '단어-등장횟수' 가방\n",
    "* TF-IDF: 문서 내 단어 등장 빈도(TF) * 문서 간 단어 등장 빈도의 역수(IDF)\n",
    "* konlpy.tag.Twitter(): 품사분류 기능의 객체 생성\n",
    "* morphs(text): text를 형태소 형태로 분류\n",
    "* pos(text): text를 (형태소, 품사)의 형태로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import response\n",
    "import numpy as np\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "PATH_1 = \"./data/criminal_law.txt\"\n",
    "PATH_2 = \"./data/military_criminal_law.txt\"\n",
    "PATH_3 = \"./data/patent_law.txt\"\n",
    "\n",
    "#다음시간에 나오는 문서 유사도 함수입니다.\n",
    "def cosine_similarity(x,y):\n",
    "    return np.dot(x,y) / (np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "BoW를 딕셔너리 형태로 출력하는 함수를 만들어 봅니다.\n",
    "'''\n",
    "def bag_of_words(tokenized_sentences): # tokenized_sentences; 세 개의 문서의 단어를 담은 리스트\n",
    "    word_dict={}\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for token in tokenized_sentence:\n",
    "            # TODO: 기존 word_dict 에 그 값이 있으면 1을 더해주고, 없으면 1을 부여합니다.\n",
    "            if token in word_dict:\n",
    "                word_dict[token] += 1\n",
    "            else:\n",
    "                word_dict[token] = 1\n",
    "                \n",
    "            # <=> word_dict[token] = word_dict.get(token, 0) + 1 \n",
    "                \n",
    "            \n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    file=open(path, 'r')\n",
    "    output=str(file.read())\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_splited_doc(path):\n",
    "    text = read_txt(path)\n",
    "    analyzer = Twitter()\n",
    "    output = analyzer.morphs(text)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "한 문서 내에서 한 단어의 등장 횟수를 출력하는 함수를 만들어 봅니다. \n",
    "'''\n",
    "def tf(doc, word):\n",
    "    return doc.count(word)\n",
    "    \n",
    "\n",
    "# 여러 문서 내에서 한 단어의 등장 횟수를 출력하는 함수입니다.. \n",
    "def idf(docs, word):\n",
    "    num=0\n",
    "    for doc in docs:\n",
    "        if doc.count(word)>0:\n",
    "            num+=1\n",
    "    return np.log(len(docs)/(1+num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 3번\n",
    "함수 tf와 idf를 활용하여 tf_idf 함수를 만들어 봅니다.\n",
    "'''\n",
    "def tf_idf(docs, bow):\n",
    "    len_vector= len(bow)\n",
    "    vectors=[]\n",
    "    keys = list(bow.keys())\n",
    "    for doc in docs:\n",
    "        vector = []\n",
    "        for i,key in enumerate(keys):\n",
    "            #TODO: TF와 IDF값을 곱하여 준 후, 변수 vector에 추가하여 줍니다.\n",
    "            vector.append(tf(doc, key)*idf(docs, key))\n",
    "            \n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW():\n",
    "\n",
    "    criminal_law = get_splited_doc(PATH_1)\n",
    "    military_criminal_law = get_splited_doc(PATH_2)\n",
    "    patent_law = get_splited_doc(PATH_3)\n",
    "\n",
    "    total = [criminal_law, military_criminal_law, patent_law]\n",
    "\n",
    "    bow = bag_of_words(total)\n",
    "\n",
    "    vecs_tfIdf = tf_idf(total, bow)\n",
    "    \n",
    "    criminal, military_criminal, patent = vecs_tfIdf\n",
    "\n",
    "    sml_c_m = cosine_similarity(criminal, military_criminal)\n",
    "    sml_c_p = cosine_similarity(criminal, patent)\n",
    "    \n",
    "    return sml_c_m, sml_c_p\n",
    "\n",
    "sml_c_m, sml_c_p = BoW()\n",
    "\n",
    "print(\"형법과 군형법 문서의 유사도 값은 \", sml_c_m, ' 입니다.')\n",
    "print(\"형법과 특허법 문서의 유사도 값은 \", sml_c_p, ' 입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습3] 문서 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문서유사도(Text Similarity): 둘 이상의 문서 간의 주제 또는 의미가 얼마나 유사한지 나타내는 것\n",
    "* 코사인 유사도(Cosine Similarity): 내적/노름\n",
    "* 자카드 유사도(Jaccard Similarity): 교집합/합집합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "\n",
    "# (다음 장에서 살펴볼) 원-핫 인코딩을 구현하는 함수입니다.\n",
    "def one_hot(x, depth):\n",
    "    output = []\n",
    "    for i in range(depth):\n",
    "        if i == x:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def sum_list(X, depth):\n",
    "    output = np.zeros(depth)\n",
    "    for _list in X:\n",
    "        output = np.add(output, _list)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "BoW를 출력하는 함수를 만들어 봅니다.\n",
    "''' \n",
    "def bag_of_words(tokenized_sentences):\n",
    "    word_dict = {}\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for token in tokenized_sentence:\n",
    "            # 구현 방법은 지난 장에서와 동일합니다.\n",
    "            word_dict[token] = word_dict.get(token, 0) + 1 \n",
    "            \n",
    "            \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "자카드 유사도(Jaccard Similarity)를 구하는 함수를 만들어 봅니다.\n",
    "''' \n",
    "def jaccard(X, Y):\n",
    "    # TODO: 요소의 중복을 허용하는 리스트의 길이를 len_total에 저장합니다.\n",
    "    len_total = len(X+Y)\n",
    "    \n",
    "    # TODO: 요소의 중복을 허용하지 않는 리스트(두 리스트의 합집합)의 길이를 len_union에 저장합니다.\n",
    "    len_union = len(set(X+Y))\n",
    "    \n",
    "    # TODO: len_total과 len_union을 이용하여 중복된 리스트(두 리스트의 교집합)의 길이를 len_inter 에 저장합니다.\n",
    "    len_inter = len_total - len_union\n",
    "    \n",
    "    return len_inter/len_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 3번\n",
    "Numpy를 이용하여(np.dot(), np.linalg.norm()) 코사인 유사도를 계산합니다.\n",
    "''' \n",
    "def cosine(x, y):\n",
    "    return np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "\n",
    "def TextSimilarity():\n",
    "    sentences = [\"나는 어제 잠을 못 잤습니다\", \"나는 어제 밥을 굶었습니다\", \"오늘 밥은 맛이 없습니다.\", \"오늘 밥은 별로입니다.\"]\n",
    "    analyzer = Twitter()\n",
    "    tokenized = [ analyzer.morphs(sen) for sen in sentences ]\n",
    "\n",
    "    ## 토큰화된 문장을 바탕으로 자카드 유사도를 출력해 봅니다.\n",
    "    print('첫 번째 문장과 두 번째 문장의 자카트 유사도는 %f 입니다.' %(jaccard(tokenized[0], tokenized[1])))\n",
    "    print('첫 번째 문장과 세 번째 문장의 자카트 유사도는 %f 입니다.' % (jaccard(tokenized[0], tokenized[2])))\n",
    "    print('두 번째 문장과 세 번째 문장의 자카트 유사도는 %f 입니다.' % (jaccard(tokenized[1], tokenized[2])))\n",
    "    ## 토큰화된 문장을 바탕으로 BoW를 만듭니다.\n",
    "\n",
    "    BoW = bag_of_words(tokenized)\n",
    "    print(\"<전체 문장에 대한 BoW>\\n\",BoW)\n",
    "\n",
    "    ## {'word':index} 딕셔너리를 만듭니다.\n",
    "    word_index = { k:v for v,k in enumerate(BoW.keys())}\n",
    "\n",
    "    ## word_index_dict의 길이를 구하고 이를 바탕으로 원-핫 인코딩을 진행합니다.\n",
    "    len_wordIndex = len(word_index)\n",
    "\n",
    "    sen1 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[0]]\n",
    "    sen2 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[1]]\n",
    "    sen3 = [one_hot(word_index[token], len_wordIndex) for token in tokenized[2]]\n",
    "\n",
    "    ## 각각의 단어별 임베딩된 것을 더하여 문장의 임베딩 값으로 나타냅니다.\n",
    "    sen1_onehot = sum_list(sen1, len_wordIndex)\n",
    "    sen2_onehot = sum_list(sen2, len_wordIndex)\n",
    "    sen3_onehot = sum_list(sen3, len_wordIndex)\n",
    "\n",
    "    print(\"첫 번째 문장의 벡터:\",sen1_onehot)\n",
    "    print(\"두 번째 문장의 벡터:\", sen2_onehot)\n",
    "    print(\"세 번째 문장의 벡터:\", sen3_onehot)\n",
    "\n",
    "    ## 이를 바탕으로 코사인 유사도를 출력합니다.\n",
    "    sm_12 = cosine(sen1_onehot, sen2_onehot)\n",
    "    sm_13 = cosine(sen1_onehot, sen3_onehot)\n",
    "\n",
    "    print(\"첫 번째 문장과 두 번째 문장의 코사인 유사도는 %f 입니다.\" %sm_12)\n",
    "    print(\"첫 번째 문장과 두 번째 문장의 코사인 유사도는 %f 입니다.\" %sm_13)\n",
    "    print(tokenized)\n",
    "    print(str(sen1_onehot))\n",
    "    print(f\"{sen1_onehot}, {sen2_onehot}, {sen3_onehot}\")\n",
    "    print(sm_12, sm_13)\n",
    "    return tokenized, sen1_onehot, sen2_onehot, sen3_onehot, sm_12, sm_13\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TextSimilarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습4] 단어 임베딩과 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단어 임베딩(Word Embedding): 문장의 단어들을 컴퓨터가 연산할 수 있는 공간(벡터공간)으로 사상(mapping)하는 것\n",
    "* 원-핫 인코딩(One-hot Encoding): 각 단어를 한 차원으로 설정하여 모든 단어의 개수를 차원으로 한 벡터를 생성한 뒤, 특정 단어에 대하여 그 위치의 차원 값을 1로 하고 나머지 차원은 0으로 만들어 주는 것\n",
    "* tokenizer.fit_on_texts(sentence): sentence에 존재하는 리스트 요소(단어)마다 고유 인덱스를 붙이는 작업\n",
    "* tokenizer.word_index: 위 작업의 결과를 딕셔너리 형태로 반환\n",
    "* tokenizer.texts_to_sequences(sentence): 문자열을 정수 인덱스의 리스트로 변환 후 반환\n",
    "* tf.one_hot(sen, len(word_dict)): sen(리스트)를 텐서플로우를 사용하여 원-핫 인코딩하는 메소드, word_dict 안의 word의 총 개수가 원-핫 벡터의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "embedding 함수를 완성합니다.\n",
    "\n",
    "   Step01. 입력된 리스트 'sentence1+sentence2'에 존재하는\n",
    "           요소마다 고유 인덱스를 붙입니다.\n",
    "           \n",
    "   Step02. 요소와 인덱스를 짝지은 딕셔너리 'word_dict'를 정의합니다.\n",
    "   \n",
    "   Step03. 'sentence1', 'sentence2'를 정수값으로 변환하고 이를\n",
    "           각각 리스트 변수 'sen1', 'sen2'로 정의합니다.\n",
    "'''\n",
    "\n",
    "def embedding(sentence1, sentence2):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence1+sentence2)\n",
    "    word_dict = tokenizer.word_index\n",
    "    \n",
    "    sen1 = tokenizer.texts_to_sequences(sentence1)\n",
    "    sen2 = tokenizer.texts_to_sequences(sentence2)\n",
    "    \n",
    "    sen1 = [token[0] for token in sen1]\n",
    "    sen2 = [token[0] for token in sen2]\n",
    "    \n",
    "    return word_dict, sen1, sen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "텐서플로우를 사용하여 원-핫 인코딩을 실행합니다.\n",
    "'''   \n",
    "\n",
    "def one_hot(sen1, sen2, word_dict):\n",
    "    \n",
    "    # 앞 칸은 빈칸 \n",
    "    oh_sen1 = sum(tf.one_hot(sen1, len(word_dict))) # -> +1\n",
    "    oh_sen2 = sum(tf.one_hot(sen2, len(word_dict))) # -> +1\n",
    "    \n",
    "    return oh_sen1, oh_sen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    sentence1 = ['나','는','오늘','저녁','에','치킨','을','먹','을','예정','입니다']\n",
    "    sentence2 = ['나','는','어제', '맥주','와', '함께', '치킨','을', '먹었', '습니다']\n",
    "    \n",
    "    word_dict, seq_1, seq_2 = embedding(sentence1, sentence2)\n",
    "    onehot_sen1, onehot_sen2 = one_hot(seq_1, seq_2, word_dict)\n",
    "        \n",
    "    print('리스트 요소-인덱스 딕셔너리: ', word_dict)\n",
    "    \n",
    "    print('\\n정수값으로 변환된 sentence1:', seq_1)\n",
    "    print('\\n정수값으로 변환된 sentence2:', seq_2)\n",
    "    \n",
    "    print('\\n원-핫 인코딩된 문장1:', onehot_sen1.numpy())\n",
    "    print('\\n원-핫 인코딩된 문장2:', onehot_sen2.numpy())\n",
    "    \n",
    "    return onehot_sen1, onehot_sen2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습5] Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec: 단어(word)를 벡터(vector)로 변환하는 것 ex. CBOW, Skip-Gram\n",
    "* CBOW 모델: 주변 단어(context word)를 바탕으로 특정 단어(target word)를 예측하는 모델\n",
    "* Skip-Gram 모델: 특정 단어(target word)를 바탕으로 주변 단어(context)를 예측하는 모델\n",
    "* from gensim.models import word2vec: gensim 라이브러리로부터 Word2Vec 모델 불러오기\n",
    "* word2vec.Word2Vec(data, size, min_count, window, sg): Word2Vec 모델을 정의하는 변수\n",
    "\n",
    " -data: 리스트 형태의 데이터\n",
    " \n",
    " -size: 문자 벡터 차원 수\n",
    " \n",
    " -min_count: 사용할 단어의 최소 빈도수\n",
    " \n",
    " -window: 고려할 앞뒤 단어 수\n",
    " \n",
    " -sg: 0은 CBOW, 1은 Skip-gram\n",
    " \n",
    "* model.wv.index2word: Word2Vec 모델인 model에 리스트 데이터 data를 넣은 결과를 반환해주는 변수\n",
    "* 한국어 word2vec 사이트; https://word2vec.kr/search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import list_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "   CBOW 방식의 Word2Vec 모델을\n",
    "   반환하는 CBOW 함수를 완성하세요.\n",
    "'''\n",
    "\n",
    "def CBOW(sentences):\n",
    "    \n",
    "    model_cbow = word2vec.Word2Vec(sentences, size=300, min_count=1, window=10, sg=0)\n",
    "    \n",
    "    return model_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "   Skip-Gram 방식의 Word2Vec 모델을\n",
    "   반환하는 Skip_Gram 함수를 완성하세요.\n",
    "'''\n",
    "\n",
    "def Skip_Gram(sentences):\n",
    "    \n",
    "    model_skipgram = word2vec.Word2Vec(sentences, size=300, min_count=1, window=10, sg=1)\n",
    "    \n",
    "    return model_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 3번\n",
    "   각 모델의 결괏값을 정의하세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    sen1, sen2 = list_file.sen1(), list_file.sen2()\n",
    "    \n",
    "    sentences = [sen1, sen2]\n",
    "    \n",
    "    cbow = CBOW(sentences) # model\n",
    "    skipgram = Skip_Gram(sentences) # model\n",
    "    \n",
    "    idx2word_set_cbow = cbow.wv.index2word\n",
    "    idx2word_set_skipgram = skipgram.wv.index2word\n",
    "    \n",
    "    print('CBOW: ', idx2word_set_cbow)\n",
    "    print('\\nSkip-Gram: ', idx2word_set_skipgram)\n",
    "    \n",
    "    return idx2word_set_cbow, idx2word_set_skipgram\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [미션] 문서 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 세 개의 문학 작품(김유정-봄봄, 김유정-소낙비, 채만식-소설안쓰는변명)에 대하여 간단하게 문서 분석 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import response\n",
    "from konlpy.tag import Twitter \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_stopwords():\n",
    "    with open('./data/stopwords.pickle', 'rb') as handle:\n",
    "        out = pickle.load(handle)\n",
    "    return out\n",
    "    \n",
    "def load_data():\n",
    "    DATA_PATH= './data/'\n",
    "    FILES = ['김유정_봄봄.txt', '김유정_소낙비.txt', '채만식_소설안쓰는변명.txt']\n",
    "    novels=[]\n",
    "    for file_name in FILES:\n",
    "        with open(DATA_PATH+file_name,'r') as f:\n",
    "            novels.append((f.read()))\n",
    "    return novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "불용어(stopwords)를 제거한 BoW를 출력하는 함수를 만들어 봅니다.\n",
    "'''\n",
    "def bag_of_words(tokenized, stopwords):\n",
    "    word_dict={}\n",
    "    for token in tokenized:\n",
    "        if token in word_dict:\n",
    "            word_dict[token] += 1\n",
    "        else:\n",
    "            word_dict[token] = 1\n",
    "        \n",
    "        \n",
    "    for stop in stopwords:\n",
    "        if stop in word_dict:\n",
    "            del word_dict[stop]\n",
    "        \n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "문서를 형태소 단위로 토큰화된 리스트로 출력하는 함수를 만들어 봅니다.\n",
    "'''\n",
    "def tokenizer(document):\n",
    "    analyzer=Twitter()\n",
    "    tokenized_documents=analyzer.morphs(document)\n",
    "    return tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 3번\n",
    "빈도 상위 k개를 출력하는 함수를 만들어 봅니다. \n",
    "'''\n",
    "def top_k(dic_bow,k):\n",
    "    # dic.get(key); key에 해당하는 value를 가져옴\n",
    "    # sorted(sort할 자료, key=정렬 기준-함수)\n",
    "    # dic_bow.get <=> lambda x: dic[x]\n",
    "    top_k_words=sorted(dic_bow, key=dic_bow.get, reverse=True)[:k]\n",
    "    return top_k_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 4번\n",
    "TF 함수를 구현하여 봅니다.\n",
    "'''\n",
    "def tf(doc, word):\n",
    "    return doc.count(word)\n",
    "    \n",
    "def idf(docs, word):\n",
    "    num=0\n",
    "    for doc in docs:\n",
    "        if doc.count(word)>0:\n",
    "            num+=1\n",
    "    return np.log(len(docs)/(1+num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 5번\n",
    "IDF 함수를 구현하여 봅니다.\n",
    "'''\n",
    "def tf_idf(docs, bow):\n",
    "    len_vector= len(bow)\n",
    "    vectors=[]\n",
    "    keys = list(bow.keys())\n",
    "    for doc in docs:\n",
    "        vector = []\n",
    "        for i,key in enumerate(keys):\n",
    "            vector.append(tf(doc, key)*idf(docs, key))\n",
    "        vectors.append(vector)\n",
    "        \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    stopwords = load_stopwords()\n",
    "    novel1, novel2, novel3 = load_data()\n",
    "    stopwords=stopwords+[\"(\", \")\",\".\",\",\", \"그것\", \"도\", \"는\", \"은\", \"다\", \"이\", \"고\", \"서\"]\n",
    "    ## 각 문서의 토큰화를 진행합니다.\n",
    "    novel1_token = tokenizer(novel1)\n",
    "    novel2_token = tokenizer(novel2)\n",
    "    novel3_token = tokenizer(novel3)\n",
    "    \n",
    "    ## 토큰화된 문서를 바탕으로 각각의 BoW를 만듭니다.\n",
    "    novel1_bow = bag_of_words(novel1_token, stopwords)\n",
    "    novel2_bow = bag_of_words(novel2_token, stopwords)\n",
    "    novel3_bow = bag_of_words(novel3_token, stopwords)\n",
    "    \n",
    "    ## 토큰화된 문서를 바탕으로 각각의 BoW를 만듭니다.\n",
    "    novel1_top_k = top_k(novel1_bow,10)\n",
    "    novel2_top_k = top_k(novel2_bow,10)\n",
    "    novel3_top_k = top_k(novel3_bow,10)\n",
    "    \n",
    "    ## 토큰화된 문서를 하나의 리스트 안에 (리스트 형태로)추가하여 전체 토큰화 문서 리스트를 만들어 줍니다.\n",
    "    total_tokens = [novel1_token, novel2_token, novel3_token]\n",
    "    ## TODO:각 문서별 BoW를 병합하여 total_bow를 만들어 줍니다.\n",
    "    total_bow=novel1_bow.copy()\n",
    "    total_bow.update(novel2_bow)\n",
    "    total_bow.update(novel3_bow)\n",
    "    \n",
    "    \n",
    "    novel1_tfidf, novel2_tfidf, novel3_tfidf = tf_idf(total_tokens, total_bow)\n",
    "    \n",
    "    return novel1, novel2, novel3, stopwords, novel1_token, novel1_bow, novel1_tfidf\n",
    "    \n",
    "if __name__ ==\"__main__\":\n",
    "    main()\n",
    "    response.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
