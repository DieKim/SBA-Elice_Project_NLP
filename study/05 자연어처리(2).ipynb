{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습1] RNN모델 파이썬으로 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 순환신경망(RNN): 시계열 자료 학습에 적합, 기계번역, 자율주행 자동차 등\n",
    "* 순환신경망 모델의 핵심: 이전 데이터를 현재 데이터의 계산에 반영하는 것\n",
    "* a = np.zeros((3,)): 내부 요소가 모두 0인 (3,) 모양의 a 벡터를 만드는 코드\n",
    "* b = np.ones((2,5)): 내부 요소가 모두 1인 (2,5) 모양의 b 벡터를 만드는 코드\n",
    "* c = np.random.random((4,)): 내부 요소가 모두 임의의 값인 (4,) 모양의 c 벡터를 만드는 코드\n",
    "* d = np.dot(i, j): i벡터와 j벡터를 내적해 d로 정의하는 코드\n",
    "* e = np.tanh(u): u 값을 tanh 함수에 적용시켜 e 로 반환하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "'''\n",
    "지시사항 1번\n",
    "간단한 RNN 모델을 만들어봅시다.\n",
    "\n",
    "   Step01. 0의 값을 갖는 (output_size,) 모양의 'state' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step02. 1의 값을 갖는 (output_size, input_size) 모양의 'w' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step03. 1의 값을 갖는 (output_size, output_size) 모양의 'u' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step04. 임의의 값을 갖는 (output_size,) 모양의 'b' 벡터를 만들어 봅니다.\n",
    "   \n",
    "   Step05. bias 가 False 이면 b를 (output_size,) 모양의 영벡터로 만듭니다.\n",
    "   \n",
    "   Step06. Numpy를 사용해서 w와 _input을 내적하고, u 와 state를 내적한 후\n",
    "           b를 더한 다음 tanh 함수를 적용합니다.\n",
    "'''\n",
    "\n",
    "def rnn(inputs, input_size, output_size, bias = False):\n",
    "    \n",
    "    input_size = len(inputs[0])\n",
    "    \n",
    "    state = np.zeros((output_size,))\n",
    "    \n",
    "    w = np.ones((output_size, input_size))\n",
    "    \n",
    "    u = np.ones((output_size, output_size))\n",
    "    \n",
    "    b = np.random.random((output_size,))\n",
    "    \n",
    "    if not bias:\n",
    "        b = np.zeros((output_size,))\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for _input in inputs:\n",
    "        \n",
    "        _output = np.tanh(np.dot(w,_input)+np.dot(u,state)+b)\n",
    "        outputs.append(_output)\n",
    "        state=_output\n",
    "        \n",
    "    return np.stack(outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------CASE 1-----------------\n",
      "\n",
      "CASE 1_a: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "CASE 1_b: [[0.27139524]\n",
      " [0.50034378]\n",
      " [0.65196748]\n",
      " [0.73075091]\n",
      " [0.76539792]]\n",
      "\n",
      "-----------------CASE 2-----------------\n",
      "\n",
      "CASE 2_a: [[0.76159416]\n",
      " [0.94268079]\n",
      " [0.95974603]\n",
      " [0.96107045]\n",
      " [0.96117143]]\n",
      "\n",
      "CASE 2_b: [[0.95125152]\n",
      " [0.99257296]\n",
      " [0.99316006]\n",
      " [0.99316806]\n",
      " [0.99316817]]\n",
      "\n",
      "-----------------CASE 3-----------------\n",
      "\n",
      "CASE 3_a: [[0.76159416 0.76159416]\n",
      " [0.9982604  0.9982604 ]\n",
      " [0.99990857 0.99990857]\n",
      " [0.99998771 0.99998771]\n",
      " [0.99999834 0.99999834]]\n",
      "\n",
      "CASE 3_b: [[0.93165065 0.94941875]\n",
      " [0.9997775  0.99983684]\n",
      " [0.99997624 0.99998258]\n",
      " [0.99999679 0.99999764]\n",
      " [0.99999957 0.99999968]]\n"
     ]
    }
   ],
   "source": [
    "# 케이스에 따라 RNN 모델의 결과가 어떻게 바뀌는지 확인해보세요.\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\"-----------------CASE 1-----------------\")\n",
    "    _input1 = [[0], [0], [0], [0], [0]]\n",
    "    \n",
    "    # 입력이 모두 0이고 출력 벡터의 크기가 1일 때 값의 추세가 어떠한지 확인해보세요.\n",
    "    case1_a = rnn(_input1, input_size=1, output_size=1)\n",
    "    print('\\nCASE 1_a:', case1_a)\n",
    "    # Bias가 있으면 값이 어떻게 변화하는지 확인해보세요.\n",
    "    case1_b = rnn(_input1, input_size=1, output_size=1, bias = True)\n",
    "    print('\\nCASE 1_b:', case1_b)\n",
    "    \n",
    "    \n",
    "    print(\"\\n-----------------CASE 2-----------------\")\n",
    "    _input2 = [[1], [1], [1], [1], [1]]\n",
    "    \n",
    "    # 입력이 모두 1이고 출력 벡터의 크기가 1일 때 값의 추세가 어떠한지 확인해보세요.\n",
    "    case2_a = rnn(_input2, input_size=1, output_size=1)\n",
    "    print('\\nCASE 2_a:', case2_a)\n",
    "    # Bias가 있으면 값이 어떻게 변화하는지 확인해보세요.\n",
    "    case2_b = rnn(_input2, input_size=1, output_size=1, bias = True)\n",
    "    print('\\nCASE 2_b:', case2_b)\n",
    "    \n",
    "    \n",
    "    print(\"\\n-----------------CASE 3-----------------\")\n",
    "    _input3 = [[1], [2], [3], [4], [5]]\n",
    "    \n",
    "    # 입력값이 증가하고 출력 벡터의 크기가 2일 때 값의 추세가 어떠한지 확인해보세요.\n",
    "    case3_a = rnn(_input3, input_size=1, output_size=2)\n",
    "    print('\\nCASE 3_a:', case3_a)\n",
    "    # Bias가 있으면 값이 어떻게 변화하는지 확인해보세요.\n",
    "    case3_b = rnn(_input3, input_size=1, output_size=2, bias = True)\n",
    "    print('\\nCASE 3_b:', case3_b)\n",
    "    \n",
    "    return case1_a, case1_b, case2_a, case2_b, case3_a, case3_b\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습2] Keras를 활용한 RNN모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 영화 리뷰 데이터를 바탕으로 감정 분석 모델 학습하기\n",
    "* IMDB 영화 리뷰 데이터 셋 -> 긍/부정 분류\n",
    "* 자연어 자료: 단어의 연속적인 배열, 즉 시계열 자료 -> RNN모델 이용\n",
    "* RNN in Keras: Embedding Layer(입력층) -> RNN Layer -> Dense Layer\n",
    "* tf.keras.layers.Embedding(input_dim, output_dim, input_length): 들어온 문장을 단어 임베딩 하는 레이어\n",
    " \n",
    " -input_dim: 들어올 단어의 개수\n",
    " \n",
    " -output_dim: 결과로 나올 임베딩 벡터의 크기(차원)\n",
    " \n",
    " -input_length: 들어오는 단어 벡터의 크기(차원)\n",
    " \n",
    "\n",
    "* tf.keras.layers.SimpleRNN(units, activation): 단순 RNN 레이어\n",
    "* tf.keras.layers.LSTM(units, activation): LSTM 레이어\n",
    "* tf.keras.layers.GRU(units, activation): GRU 레이어\n",
    "* Simple RNN: LSTM이나 GRU에 비해 학습 속도는 빠르나 문장의 길이가 긴 경우 기울기 소실(Vanishing Gradient) 문제로 학습이 잘 되지 않을 것이며 문장 끝 몇 단어에 크게 영향을 받거나 과적합이 우려될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# 데이터를 불러오고 전처리하는 함수입니다.\n",
    "\n",
    "def load_data(n_of_training_ex, n_of_testing_ex, max_review_length):\n",
    "    \n",
    "    PATH = \"./data/\"\n",
    "    \n",
    "    X_train = np.load(PATH + \"X_train.npy\")[:n_of_training_ex]\n",
    "    y_train = np.load(PATH + \"y_train.npy\")[:n_of_training_ex]\n",
    "    X_test = np.load(PATH + \"X_test.npy\")[:n_of_testing_ex]\n",
    "    y_test = np.load(PATH + \"y_test.npy\")[:n_of_testing_ex]\n",
    "    \n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "SimpleRNN을 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "    \n",
    "def SimpleRNN(embedding_vector_length, max_review_length):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(1000, embedding_vector_length, input_length=max_review_length))\n",
    "    model.add(tf.keras.layers.SimpleRNN(5))\n",
    "    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2번\n",
    "LSTM을 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "def LSTM(embedding_vector_length, max_review_length):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(1000, embedding_vector_length, input_length=max_review_length))\n",
    "    model.add(tf.keras.layers.LSTM(5))\n",
    "    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 3번\n",
    "GRU를 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "def GRU(embedding_vector_length, max_review_length):\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(1000, embedding_vector_length, input_length=max_review_length))\n",
    "    model.add(tf.keras.layers.GRU(5))\n",
    "    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 4번\n",
    "세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. SimpleRNN, LSTM, GRU 함수를 이용해 세 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 세 모델을 각각 학습시킵니다. 검증용 데이터는 설정하지 않습니다.\n",
    "           세 모델 모두 'epochs'는 3, 'batch_size'는 256으로 설정합니다.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 각각의 Test Accuracy 값을 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    max_review_length = 300\n",
    "    embedding_vector_length = 32\n",
    "    \n",
    "    n_of_training_ex = 25000\n",
    "    n_of_testing_ex = 3000\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_data(n_of_training_ex, n_of_testing_ex, max_review_length)\n",
    "    \n",
    "    \n",
    "    # 모델 만들기\n",
    "    model_simple_rnn = SimpleRNN(embedding_vector_length, max_review_length)\n",
    "    model_lstm = LSTM(embedding_vector_length, max_review_length)\n",
    "    model_gru = GRU(embedding_vector_length, max_review_length)\n",
    "    \n",
    "    # 컴파일\n",
    "    model_simple_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    # 모델 구조 출력하기\n",
    "    model_simple_rnn.summary()\n",
    "    model_lstm.summary()\n",
    "    model_gru.summary()\n",
    "    \n",
    "    # 모델 학습시키기\n",
    "    model_simple_rnn_history = model_simple_rnn.fit(X_train, y_train, epochs = 3, batch_size=256)\n",
    "    model_lstm_history = model_lstm.fit(X_train, y_train, epochs = 3, batch_size=256)\n",
    "    model_gru_history = model_gru.fit(X_train, y_train, epochs = 3, batch_size=256)\n",
    "    \n",
    "    # 모델 평가하기\n",
    "    scores_simple_rnn = model_simple_rnn.evaluate(X_test, y_test)\n",
    "    scores_lstm = model_lstm.evaluate(X_test, y_test)\n",
    "    scores_gru = model_gru.evaluate(X_test, y_test)\n",
    "    \n",
    "    print('\\nTest Accuracy_simple rnn: ', scores_simple_rnn[-1])\n",
    "    print('Test Accuracy_lstm: ', scores_lstm[-1])\n",
    "    print('Test Accuracy_gru: ', scores_gru[-1])\n",
    "    \n",
    "    return model_simple_rnn_history, model_lstm_history, model_gru_history\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습3] LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTM을 TensorFlow를 이용하여 구현하고 이를 바탕으로 IMDB 영화리뷰 감정 분석을 진행하기\n",
    "* LSTM(Long short-term memory): 기존 RNN에 장단기 기억을 가능하게 하는 cell state를 추가한 모델\n",
    "* 기존 RNN의 장기 의존성(Long-Terms Dependencies) 문제를 해결하기 위해 등장한 RNN 기반의 신경망\n",
    "* tf.keras.layers.LSTM(units): LSTM 레이어\n",
    "* tf.keras.layers.Lambda(function): 다음 층과의 계산을 위해 출력의 차원을 늘려줌\n",
    "* tf.keras.layers.GlobalMaxPool1D(): Global MaxPooling 층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 6000\n",
    "max_len =  130\n",
    "embedding_size = 128\n",
    "\n",
    "(input_train, label_train), (input_test, label_test) = imdb.load_data(num_words=max_features)\n",
    "input_train = pad_sequences(input_train, maxlen=max_len)\n",
    "input_test = pad_sequences(input_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 1번\n",
    "텐서 데이터의 차원을 1차원 늘리는 함수를 만들어 봅니다.\n",
    "''' \n",
    "# TODO: 텐서 데이터의 차원을 1차원 늘리는 함수를 만들어 봅니다. \n",
    "def expand_dim(_input):\n",
    "    ## EX) (3,2) -> (3,2,1) 과 같이 뒤쪽에 차원을 늘림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "지시사항 2~4번\n",
    "2. tensorflow.keras를 활용하여 LSTM 레이어 층을 추가합니다.\n",
    "3. 다음 층과의 계산을 위해 Lambda를 활용하여 출력의 차원을 늘려줍니다. \n",
    "4. Global Max Pooling 층을 추가 합니다.\n",
    "''' \n",
    "def main():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features, embedding_size),\n",
    "    # TODO: tensorflow.keras를 활용하여 LSTM 레이어 층을 추가합니다.\n",
    "    tf.keras.layers.LSTM(32, activation= tf.nn.tanh),\n",
    "    # TODO: 다음 층과의 계산을 위해 Lambda를 활용하여 출력의 차원을 늘려줍니다. \n",
    "    tf.keras.layers.Lambda(expand_dim),\n",
    "    # TODO: Global Max Pooling 층을 추가 합니다.\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.05),\n",
    "    tf.keras.layers.Dense(1, activation = tf.nn.sigmoid),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    #모델 학습하기\n",
    "    model_history= model.fit(input_train, label_train, validation_split=.2, batch_size=100, epochs=10)\n",
    "\n",
    "\n",
    "    # 테스트데이터 예측하기\n",
    "    prediction = model.predict(input_test)\n",
    "    print(\"각 예측의 결과는: \", prediction)\n",
    "    print(\"각 예측의 결과를 binary로 표현하면: \", tf.round(prediction))\n",
    "    y_pred = tf.round(prediction)\n",
    "\n",
    "    # 예측결과 평가하기\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "    m.update_state(label_test, y_pred)\n",
    "    print(m.result().numpy())\n",
    "\n",
    "    return model_history\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습4] CNN을 활용한 문서분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNN과 RNN을 TensorFlow를 이용하여 구현하고 이를 바탕으로 IMDB 영화리뷰 감정 분석을 진행하기\n",
    "* CNN: 커널을 바탕으로 합성곱 연산을 진행하는 인공신경망, 주로 영상이나 이미지 특징 추출에 사용\n",
    "* CNN을 자연어 문장의 특징을 추출하는데 적용할 수 있음, 성능도 꽤 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 6000\n",
    "max_len =  130\n",
    "embedding_size = 128\n",
    "\n",
    "(input_train, label_train), (input_test, label_test) = imdb.load_data(num_words=max_features)\n",
    "input_train = pad_sequences(input_train, maxlen=max_len)\n",
    "input_test = pad_sequences(input_test, maxlen=max_len)\n",
    "\n",
    "# TODO: 텐서 데이터에 대해 Convolution 연산을 진행하고 출력하는 함수를 만들어 봅니다.\n",
    "def conv(_input, node=16):\n",
    "    _input = tf.cast(_input, tf.float32)\n",
    "    _input = tf.expand_dims(_input,-1)\n",
    "    conv1=tf.keras.layers.Conv1D(node, kernel_size=1, padding='SAME', activation=tf.nn.relu, dtype=tf.float32)\n",
    "    conv2=tf.keras.layers.Conv1D(node, kernel_size=2, padding='SAME', activation=tf.nn.relu, dtype=tf.float32)\n",
    "    conv3=tf.keras.layers.Conv1D(node, kernel_size=3, padding='SAME', activation=tf.nn.relu, dtype=tf.float32)\n",
    "    conv1 = conv1(_input)\n",
    "    conv2 = conv2(_input)\n",
    "    conv3 = conv3(_input)\n",
    "    concat = tf.keras.layers.concatenate([conv1, conv2, conv3])\n",
    "    return concat\n",
    "\n",
    "def expand_dim(_input):\n",
    "    return tf.expand_dims(_input, -1)\n",
    "\n",
    "def main():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_features, embedding_size),\n",
    "    # TODO: LSTM 기반의 BiRNN층을 추가합니다.\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, activation= tf.nn.relu, return_sequences = True)),\n",
    "    tf.keras.layers.GlobalMaxPool1D(),\n",
    "    # TODO: Lambda 함수를 활용하여 conv 함수를 레이어로 추가시켜 줍니다.\n",
    "    tf.keras.layers.Lambda(conv),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation = tf.nn.sigmoid, dtype=tf.float32),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "    \n",
    "    #모델 학습하기\n",
    "    model_history = model.fit(input_train, label_train, validation_split=.2, batch_size=100, epochs=5)\n",
    "\n",
    "\n",
    "    # 테스트데이터 예측하기\n",
    "    prediction = model.predict(input_test)\n",
    "    print(\"각 예측의 결과는: \", prediction)\n",
    "    print(\"각 예측의 결과를 binary로 표현하면: \", tf.round(prediction))\n",
    "    y_pred = tf.round(prediction)\n",
    "\n",
    "    # 예측결과 평가하기\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "    m.update_state(label_test, y_pred)\n",
    "    print(m.result().numpy())\n",
    "    \n",
    "    return model_history\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
