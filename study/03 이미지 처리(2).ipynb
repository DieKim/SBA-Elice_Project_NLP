{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습1] LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LeNet: 최초의 CNN 모델\n",
    "* Fully Connected Layer(MLP)의 한계를 극복하고자 Convolution 연산을 처음으로 도입한 인공신경망\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet Model\n",
    "def LeNet():\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    '''\n",
    "    지시사항 1번\n",
    "    LeNet 구조를 완성하세요.\n",
    "    '''\n",
    "    # Conv 1 Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, activation='relu', input_shape=(32,32,1)))\n",
    "    \n",
    "    # Sub Sampling Layer (Max Pooling)\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Conv 1 Layer\n",
    "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, activation='relu', input_shape=(16,16,1)))\n",
    "    \n",
    "    # Sub Sampling Layer (Max Pooling)\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Fully Connected (FC) Layer와 연결하기 위한 Flatten\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # FC1 Layer \n",
    "    model.add(tf.keras.layers.Dense(units=120, activation='relu'))\n",
    "    \n",
    "    # FC2 Layer\n",
    "    model.add(tf.keras.layers.Dense(units=84, activation='relu'))\n",
    "    \n",
    "    # Output Softmax\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    \n",
    "lenet = LeNet()\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습2] VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VGGNet:모델의 깊이에 따른 변화를 비교할 수 있게 만든 모델\n",
    "* 모든 Conv Layer에 3 * 3 필터를 사용한 것이 특징\n",
    "* 이전까지의 모델은 첫 번째 Conv Layer에서는 입력 영상의 축소를 위해 11 * 11, 7 * 7 등의 필터를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16():\n",
    "    # Sequential 모델 선언\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    '''\n",
    "    지시사항 1번\n",
    "    3 x 3 convolution만을 사용하여 VGG16 Net을 완성하세요.\n",
    "    '''\n",
    "    # 첫 번째 Conv Block\n",
    "    # 입력 Shape는 ImageNet 데이터 세트의 크기와 같은 RGB 영상 (224 x 224 x 3)입니다.\n",
    "    model.add(keras.layers.Conv2D(filters = 64, kernel_size = 3, activation='relu', padding='SAME', input_shape = (224, 224, 3)))\n",
    "    model.add(keras.layers.Conv2D(filters = 64, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    # 두 번째 Conv Block\n",
    "    model.add(keras.layers.Conv2D(filters = 128, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 128, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    # 세 번째 Conv Block\n",
    "    model.add(keras.layers.Conv2D(filters = 256, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 256, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 256, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    # 네 번째 Conv Block\n",
    "    model.add(keras.layers.Conv2D(filters = 512, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 512, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 512, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    # 다섯 번째 Conv Block\n",
    "    model.add(keras.layers.Conv2D(filters = 512, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 512, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.Conv2D(filters = 512, kernel_size = 3, activation='relu', padding='SAME'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(4096, activation= tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(4096, activation= tf.nn.relu))\n",
    "    model.add(keras.layers.Dense(1000, activation= tf.nn.softmax))\n",
    "    \n",
    "    return model\n",
    "\n",
    "vgg16 = VGG16()\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습3] ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ResNet: 굉장히 깊은 층(최대 152-Layer)까지 쌓을 수 있는 고성능 모델\n",
    "* Degradation Problem: 모델의 층이 깊어질 수록 Backpropagation에서 기울기가 0으로 수렴해서 학습이 진행되지 않는 Gradient Vanishing 현상\n",
    "* ResNet은 Degradation Problem을 완화하고 깊은 Layer를 가진 모델을 만들기 위해 'Skip Connection'이란 Residual Learning 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import add, Input,Dense,Activation, Flatten, Conv2D, MaxPooling2D, GlobalMaxPooling2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력과 출력의 Dimension이 같은 경우 사용합니다.\n",
    "def identity_block(input_tensor, kernel_size, filters):\n",
    "    \n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    x = Conv2D(filters1, (1, 1))(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    '''\n",
    "    지시사항 1번\n",
    "    아래 내용을 채워 identity_block()을 완성하세요.\n",
    "    '''\n",
    "    # 입력(x) : input_tensor와 F(x) : x를 더해줍니다.\n",
    "    # TODO : add()와 Activation() 메서드를 사용해서 relu(F(x) + x) 의 형태로 만들어보세요. \n",
    "    x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_tensor, kernel_size, filters, strides=(2, 2)):\n",
    "    filters1 , filters2 , filters3 = filters\n",
    "    \n",
    "    # 입력 Feature Map의 Size를 1/2로 줄이는 대신 Feature map의 Dimension을 2배로 늘려줍니다.\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides)(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    지시사항 2번\n",
    "    아래 내용을 채워 residual_block()을 완성하세요.\n",
    "    '''\n",
    "    # TODO : Projection Shortcut Connection을 구현해보세요.\n",
    "    # 1 x 1 Convolution 연산을 수행하여 Dimension을 2배로 증가시키고\n",
    "    # 입력 Feature map의 size를 1/2로 축소시켜보세요.\n",
    "    shortcut = Conv2D(filters3, (1,1), strides=strides)(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    # F(x) : x와 Shortcut Connection : shortcut을 더해줍니다.\n",
    "    # TODO : add()와 Activation() 메서드를 사용해서 relu(F(x) + shortcut) 의 형태로 만들어보세요.\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50():\n",
    "    # 입력 이미지의 Shape을 정해줍니다.\n",
    "    shape = (224,224,3)\n",
    "    inputs = Input(shape)\n",
    "    \n",
    "    # 입력 영상의 크기를 줄이기 위한 Conv & Max-pooling\n",
    "    x = ZeroPadding2D((3, 3))(inputs)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "    \n",
    "    # 첫 번째 Residual Block (입력 영상 Size 2배 축소 / Dimension 2배 증가)\n",
    "    x = residual_block(x, 3, [64, 64, 256], strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "    x = identity_block(x, 3, [64, 64, 256])\n",
    "    \n",
    "    \n",
    "    # 두 번째 Residual Block (입력 영상 Size 2배 축소 / Dimension 2배 증가)\n",
    "    x = residual_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    x = identity_block(x, 3, [128, 128, 512])\n",
    "    \n",
    "    # 세 번째 Residual Block (입력 영상 Size 2배 축소 / Dimension 2배 증가)\n",
    "    x = residual_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    x = identity_block(x, 3, [256, 256, 1024])\n",
    "    \n",
    "    # 네 번째 Residual Block (입력 영상 Size 2배 축소 / Dimension 2배 증가)\n",
    "    x = residual_block(x, 3, [512, 512, 2048])\n",
    "    x = identity_block(x, 3, [512, 512, 2048])\n",
    "    x = identity_block(x, 3, [512, 512, 2048])\n",
    "\n",
    "    # 마지막단에서 FC layer를 쓰지 않고 단순히 Averaging 합니다.\n",
    "    x = AveragePooling2D((7, 7))(x)\n",
    "    x = Flatten()(x)\n",
    "    # 1000개의 Class 구분\n",
    "    x = Dense(1000, activation='softmax')(x)\n",
    "    \n",
    "    # 모델 구성\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "model = ResNet50()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습4] Deconvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolution: Feature map의 크기를 줄이고 특정 Feature를 추출하는 역할\n",
    "* Deconvolution: Feature map의 크기를 증가시키는 방향으로 동작\n",
    "* 먼저 Zero Padding을 통해 feature map의 크기를 늘리고,\n",
    "* 늘어난 feature map에 convolution 연산을 해줌\n",
    "* 즉, CNN의 결과물을 반대로 되돌려 input과 같은 사이즈를 만들 때 사용\n",
    "* ex) Segmentation, CNN Visualization 등\n",
    "* tf.keras.layers.Conv2DTranspose(): Deconvolution을 위한 함수\n",
    "\n",
    " -filters: Output filter 개수\n",
    " \n",
    " -kernel_size: Convolution kernel의 크기\n",
    " \n",
    " -padding: SAME or VALID\n",
    " \n",
    " -strides: Kernel이 움직이는 쪽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2DTranspose, MaxPooling2D, AveragePooling2D, Dropout, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 이미지의 Shape입니다.\n",
    "image_size = (256, 256, 3)\n",
    "'''\n",
    "지시사항 1번\n",
    "임의의 Feature map을 tuple 형태로 선언하세요.\n",
    "'''\n",
    "# TODO : 임의의 Feature map (16,16,64)을 만들어줍니다.\n",
    "feature_map = (16,16,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconvolution_fun():\n",
    "\n",
    "    '''\n",
    "    지시사항 2번\n",
    "    Conv2DTranspose로 Deconvolution layer를 쌓으세요.\n",
    "    '''\n",
    "    \n",
    "    # Deconvolution Model입니다.\n",
    "    Deconv_model = keras.Sequential([\n",
    "        Input(feature_map),\n",
    "        Conv2DTranspose(filters=32, kernel_size=3, padding='SAME', strides=2),\n",
    "        Conv2DTranspose(filters=16, kernel_size=3, padding='SAME', strides=2),\n",
    "        Conv2DTranspose(filters=8, kernel_size=3, padding='SAME', strides=2),\n",
    "    \n",
    "    # TODO : 마지막 Conv2DTranspose를 통해 입력 이미지의 Shape과 같은 (256,256,3)의 Shape을 만들어주세요.\n",
    "         tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, padding='SAME', strides=2)\n",
    "    ])\n",
    "        \n",
    "    # Deconvolution Model 구조 출력\n",
    "    Deconv_model.summary()\n",
    "    return Deconv_model\n",
    "\n",
    "# image_size와 Deconv_model의 결과 Shape 비교\n",
    "if image_size == deconvolution_fun().layers[-1].output_shape[1:4]:\n",
    "    print('Feature Map을 입력 이미지의 Shape과 똑같이 복원해냈습니다.')\n",
    "else:\n",
    "    print('입력 이미지의 Shape과 Output Feature map의 Shape이 다릅니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습5] Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Image Segmentation: 영상 분할\n",
    "* Semantic Segmentation: 컴퓨터 비전 분야, 사진에서 물체를 찾고 물체와 배경을 분리하는 작업\n",
    "* Classification, Detection과도 많은 연관\n",
    "* 픽셀 단위의 예측을 수행하여 대상을 분리\n",
    "* tf.keras.layers.UpSampling2D(): 영상 분할을 위한 함수\n",
    "\n",
    " -size: Upsampling을 위한 factor 값\n",
    " \n",
    " -interpolation: Upsampling 시 늘어난 feature map 안을 채울 방법(nearest or bilinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, UpSampling2D, AveragePooling2D, Dropout, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 이미지 Shape\n",
    "image_shape = (256,256,3)\n",
    "\n",
    "# 간단한 Segmetation Model입니다.\n",
    "def Segmentation():\n",
    "    shape = (256,256,3)\n",
    "    inputs = Input(shape)\n",
    "    \n",
    "    '''\n",
    "    지시시항 1번\n",
    "    Segmentation의 결과 Shape가 임의의 이미지와 같아지도록\n",
    "    모델을 구성하세요.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # TODO : 4 층의 3 x 3 Convolution Layer를 쌓아보세요. (padding = 'same', strides = 2)\n",
    "    conv1 = Conv2D(filters=16, kernel_size=3, padding='same', strides=2)(inputs)\n",
    "    conv2 = Conv2D(filters=32, kernel_size=3, padding='same', strides=2)(conv1)\n",
    "    conv3 = Conv2D(filters=64, kernel_size=3, padding='same', strides=2)(conv2)\n",
    "    conv4 = Conv2D(filters=128, kernel_size=3, padding='same', strides=2)(conv3)\n",
    "    \n",
    "    # TODO : 3 층의 1 x 1 Convolution Layer를 쌓아보세요. (padding = 'same', strides = 1)\n",
    "    conv5 = Conv2D(filters=64, kernel_size=1, padding='same', strides=1)(conv4)\n",
    "    conv6 = Conv2D(filters=32, kernel_size=1, padding='same', strides=1)(conv5)\n",
    "    conv7 = Conv2D(filters=3, kernel_size=1, padding='same', strides=1)(conv6)\n",
    "    \n",
    "    # TODO : Upsampling을 통해 image_shape와 같은 (256,256,3)의 output을 만들어보세요.\n",
    "    upsampling = UpSampling2D((16,16))(conv7)\n",
    "    \n",
    "    ## Conv2DTranspose 로도 가능\n",
    "    # upsampling = Conv2DTranspose(filters=3, kernel_size=1, padding='same', strides=16)(conv7)\n",
    "    \n",
    "    # 쌓은 Layer들을 모델로 만들어줍니다.\n",
    "    model = Model(inputs = [inputs], outputs = [upsampling])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = Segmentation()\n",
    "seg_model.summary()\n",
    "\n",
    "# image_shape와 seg_model의 결과 Shape 비교\n",
    "if image_shape == seg_model.layers[-1].output_shape[1:4]:\n",
    "    print('Segmentation을 입력 이미지의 크기와 똑같이 복원해냈습니다.')\n",
    "else:\n",
    "    print('입력 이미지의 크기와 Model Output Shape이 다릅니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습6] Transfer Learning(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전이 학습(Transfer Learning): 기존의 모델을 이용해 더 빠른 학습과 예측 성능을 높이는 방법\n",
    "* 이미 잘 학습된 모델이 있고, 이 모델과 유사한 문제를 풀 때 전이 학습 이용\n",
    "* 전이 학습을 사용하는 이유\n",
    " \n",
    " -기존에 학습이 잘 된 모델이 많음\n",
    " \n",
    " -복잡한 모델일수록 새로운 학습에 많은 연산량, 메모리, 시간이 소요됨\n",
    " \n",
    " -실질적으로 처음부터 새로운 모델을 학습시키기 어려운 경우 사용\n",
    "\n",
    "\n",
    "* 전이 학습을 구현하는데 있어 데이터의 양과 데이터의 따른 유사도\n",
    "\n",
    " -적은 양의 유사 데이터: 데이터의 양이 적기 때문에 Overfitting을 방지하기 위해 뒤쪽의 Classifier만 학습\n",
    " \n",
    " -적은 양의 다른 데이터: 데이터의 양이 적기 때문에 뒤쪽의 Classifier만 학습하지만 성능 향상을 기대하기 힘듦\n",
    " \n",
    " -많은 양의 유사 데이터: Overfitting의 위험이 적으므로 전체 및 많은 Layer를 Fine-tuning\n",
    " \n",
    " -많은 양의 다른 데이터: 데이터가 다르기 때문에 전체 모델을 Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수\n",
    "def Visulaize(histories, key='loss'):\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # MNIST 데이터 세트를 불러오고 Train과 Test를 나누어줍니다.\n",
    "    mnist = np.load('./data/mnist.npz')\n",
    "    X_train, X_test, y_train, y_test = mnist['x_train'][:5000], mnist['x_test'][:1000], mnist['y_train'][:5000], mnist['y_test'][:1000]\n",
    "\n",
    "    # Transfer Learning을 위해 MNIST 데이터를 나누어줍니다.\n",
    "    # Label값 (0 ~ 4 / 5 ~ 9)에 따라 5개씩 나누어줍니다.\n",
    "    x_mnist_04 = []\n",
    "    y_mnist_04 = []\n",
    "    x_mnist_59 = []\n",
    "    y_mnist_59 = []\n",
    "\n",
    "    for idx, label in enumerate(y_train):\n",
    "        if label <= 4:\n",
    "            x_mnist_04.append(X_train[idx])\n",
    "            y_mnist_04.append(y_train[idx])\n",
    "\n",
    "        else:\n",
    "            x_mnist_59.append(X_train[idx])\n",
    "            y_mnist_59.append(y_train[idx])\n",
    "\n",
    "    # (0 ~ 4)의 데이터로 학습하고 (5 ~ 9)의 데이터로 검증을 해보겠습니다.\n",
    "    X_train04, y_train04 = np.array(x_mnist_04), np.array(y_mnist_04)\n",
    "    X_test59, y_test59 = np.array(x_mnist_59), np.array(y_mnist_59)\n",
    "\n",
    "    # 나눈 MNIST 데이터 전처리\n",
    "    X_train04 = X_train04.astype(np.float32) / 255.\n",
    "    X_test59 = X_test59.astype(np.float32) / 255.\n",
    "\n",
    "    X_train04 = np.expand_dims(X_train04, axis=-1)\n",
    "    X_test59 = np.expand_dims(X_test59, axis=-1)\n",
    "\n",
    "    y_train04 = to_categorical(y_train04, 10)\n",
    "    y_test59 = to_categorical(y_test59, 10)\n",
    "\n",
    "    # CNN 모델 선언\n",
    "    CNN_model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu, input_shape=(28,28,1)),\n",
    "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu),\n",
    "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(32, activation=tf.nn.sigmoid),\n",
    "        keras.layers.Dense(16, activation=tf.nn.sigmoid),\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "\n",
    "    # CNN model을 학습시켜줍니다.\n",
    "    CNN_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    CNN_model.summary()\n",
    "\n",
    "    '''\n",
    "    지시사항 1,2번\n",
    "    [0 ~ 4] Label의 데이터로 `CNN_model`을 학습시키고 [5 ~ 9] Label의 데이터로 `CNN_model`을 검증해보세요.\n",
    "    '''\n",
    "    CNN_history = CNN_model.fit(X_train04, y_train04, epochs=20, batch_size=100, \n",
    "                                validation_data=(X_test59, y_test59), verbose=2)\n",
    "\n",
    "    '''\n",
    "    지시사항 3번\n",
    "    학습이 어떻게 이루어졌는지 확인합니다.\n",
    "    '''\n",
    "    Visulaize([('CNN', CNN_history)])\n",
    "\n",
    "\n",
    "    ################################################################################\n",
    "    # Transfer Learning을 위한 과정입니다.\n",
    "    # 학습된 CNN_model의 Classifier 부분인 Flatten() - Dense() layer를 제거해줍니다.\n",
    "    CNN_model.summary()\n",
    "    \n",
    "    '''\n",
    "    지시사항 4번\n",
    "    총 3개의 Dense layer와 1개의 Flatten layer가 있으므로 4번 pop을 해줍니다.\n",
    "    '''\n",
    "    \n",
    "    print(CNN_model.layers)\n",
    "    CNN_model.pop()\n",
    "    CNN_model.pop()\n",
    "    CNN_model.pop()\n",
    "    CNN_model.pop()\n",
    "\n",
    "    # Classifier를 지운 모델의 구조를 확인합니다.\n",
    "    CNN_model.summary()\n",
    "\n",
    "    # 이제 CNN_model에는 학습된 Convolution Layer만 남아있습니다.\n",
    "\n",
    "    # Convolution Layer의 학습된 Weight들을 저장합니다.\n",
    "    CNN_model.save_weights('CNN_model.h5', save_format='h5')\n",
    "    # 여기까지가 Transfer Learning의 1차 과정입니다.\n",
    "    # 다음 실습에서 이어서 Transfer Learning을 진행하겠습니다.\n",
    "\n",
    "    return CNN_model.summary()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [실습7] Transfer Learning(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transfer Learning(1)에서 저장한 Weight 파일을 불러와 새로운 Classfier을 더해 전이 학습 마무리하기\n",
    "* 새로운 Classifier을 달아 학습을 진행할 때, 이미 학습된 Convolution layer는 학습에서 제외해야 함\n",
    "* model.load_weights('*.h5'): h5 포맷의 weight를 불러옴, 이때 h5의 구조와 model의 구조가 같아야 함\n",
    "* model.layers[]: 모델의 Layer 값을 포함하고 있음. Numpy 리스트처럼 index/slicing 가능\n",
    "* model.layers.trainable: 학습을 진행할지에 대한 여부 결정(True or False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수\n",
    "def Visulaize(histories, key='loss'):\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # MNIST Data를 Train과 Test로 나누어줍니다.\n",
    "    mnist = np.load('./data/mnist.npz')\n",
    "    X_train, X_test, y_train, y_test = mnist['x_train'][:500], mnist['x_test'][:500], mnist['y_train'][:500], mnist['y_test'][:500]\n",
    "\n",
    "    # MNIST Data를 전저리합니다.\n",
    "    X_train = X_train.astype(np.float32) / 255.\n",
    "    X_test = X_test.astype(np.float32) / 255.\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    # 이전 실습에서 사용했던 CNN_model과 같은 구조를 가진 모델을 선언합니다.\n",
    "    # 저장된 Weights를 불러오기 위해서는 모델의 구조가 같아야합니다.\n",
    "    Transfer_model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu, input_shape=(28,28,1)),\n",
    "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu),\n",
    "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu)\n",
    "    ])\n",
    "\n",
    "    '''\n",
    "    지시사항 1번\n",
    "    Transfer_model 모델에 학습된 Weight를 넣어주세요.\n",
    "    '''\n",
    "    Transfer_model.load_weights('./data/CNN_model.h5')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    지시사항 2번\n",
    "    새로운 Classifier를 Transfer_model에 붙여주세요.\n",
    "    '''\n",
    "    Transfer_model.add(tf.keras.layers.Flatten())\n",
    "    Transfer_model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    Transfer_model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    Transfer_model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Transfer_model을 출력합니다.\n",
    "    Transfer_model.summary()\n",
    "\n",
    "\n",
    "    # 전체 모델에서 Classifier 부분만 학습하기 위해 Trainable 여부를 설정할 수 있습니다.\n",
    "    '''\n",
    "    지시사항 3번\n",
    "    앞의 Convolution layer는 학습에서 제외하고 뒤의 Classifier 부분만 학습하기 위해 Trainable을 알맞게 설정해주세요.\n",
    "    '''\n",
    "    for layer in Transfer_model.layers[:3]:\n",
    "        layer.trainable = False\n",
    "    for layer in Transfer_model.layers[3:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    '''\n",
    "    지시사항 4,5 번\n",
    "    test_acc 값이 0.8 이상이 되도록 Transfer_model을 세팅하고 학습합니다.\n",
    "    '''\n",
    "    # Transfer_model을 학습시켜줍니다.\n",
    "    Transfer_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "    Transfer_history = Transfer_model.fit(X_train, y_train, epochs=20, batch_size = 100, \n",
    "                                            validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "    Visulaize([('CNN', Transfer_history)])\n",
    "    \n",
    "    # evaluate 함수를 사용하여 테스트 데이터의 결과값을 저장합니다.\n",
    "    loss, test_acc = Transfer_model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "    return test_acc\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
